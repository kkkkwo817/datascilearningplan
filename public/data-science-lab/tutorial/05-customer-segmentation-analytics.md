# æ•™ç¨‹ 05: å®¢æˆ¶åˆ†ç¾¤èˆ‡æ©Ÿå™¨å­¸ç¿’å¯¦æˆ°æŒ‡å—

## ğŸ“š å­¸ç¿’ç›®æ¨™

æœ¬æ•™ç¨‹å°‡å¸¶ä½ æ·±å…¥äº†è§£å®¢æˆ¶åˆ†ç¾¤åˆ†æçš„å¯¦å‹™æ“ä½œï¼Œå¾æ©Ÿå™¨å­¸ç¿’åŸºç¤åˆ°ç­–ç•¥è½åœ°ï¼Œè®“ä½ èƒ½å¤ ï¼š

- æŒæ¡å¤šç¨®èšé¡ç®—æ³•çš„åŸç†å’Œå¯¦éš›æ‡‰ç”¨
- ç†è§£ç‰¹å¾µå·¥ç¨‹åœ¨å®¢æˆ¶åˆ†ç¾¤ä¸­çš„é—œéµä½œç”¨
- å­¸æœƒè©•ä¼°èšé¡æ•ˆæœå’Œé¸æ“‡æœ€ä½³åƒæ•¸
- å»ºç«‹å®¢æˆ¶ç¾¤çµ„æ¨™ç±¤å’Œå·®ç•°åŒ–è¡ŒéŠ·ç­–ç•¥
- å¯¦ç¾ROIé ä¼°å’Œç‡ŸéŠ·æ•ˆæœé æ¸¬

## ğŸ¯ ç‚ºä»€éº¼è¦é€²è¡Œå®¢æˆ¶åˆ†ç¾¤ï¼Ÿ

### å•†æ¥­åƒ¹å€¼
- **ç²¾æº–è¡ŒéŠ·**: é‡å°ä¸åŒå®¢æˆ¶ç¾¤çµ„åˆ¶å®šå·®ç•°åŒ–ç­–ç•¥ï¼Œæå‡è½‰æ›ç‡
- **è³‡æºå„ªåŒ–**: åˆç†åˆ†é…è¡ŒéŠ·é ç®—ï¼Œæé«˜æŠ•è³‡å›å ±ç‡
- **å®¢æˆ¶ç•™å­˜**: è­˜åˆ¥é«˜åƒ¹å€¼å®¢æˆ¶ä¸¦åˆ¶å®šç¶­è­·ç­–ç•¥
- **é¢¨éšªæ§åˆ¶**: æ—©æœŸè­˜åˆ¥æµå¤±é¢¨éšªï¼Œåˆ¶å®šæŒ½å›æ–¹æ¡ˆ

### å¯¦éš›æ¡ˆä¾‹
ä¸€å®¶é›¶å”®é€£é–ä¼æ¥­é€šéå®¢æˆ¶åˆ†ç¾¤åˆ†æç™¼ç¾ï¼š
- 15%çš„å®¢æˆ¶è²¢ç»äº†70%çš„ç‡Ÿæ”¶
- å¯¦æ–½åˆ†ç¾¤è¡ŒéŠ·å¾Œï¼Œè½‰æ›ç‡æå‡180%
- å®¢æˆ¶æµå¤±ç‡é™ä½40%ï¼Œå¹´åº¦ç¯€çœæˆæœ¬300è¬å…ƒ
- å¹³å‡è¨‚å–®åƒ¹å€¼æå‡25%ï¼Œæ•´é«”ç‡Ÿæ”¶å¢é•·35%

## ğŸ” å®¢æˆ¶åˆ†ç¾¤åˆ†ææ¡†æ¶

### 1. æ•¸æ“šæº–å‚™éšæ®µ
```
ç‰¹å¾µæ”¶é›† â†’ æ•¸æ“šæ¸…ç† â†’ ç‰¹å¾µå·¥ç¨‹ â†’ æ•¸æ“šæ¨™æº–åŒ–
```

### 2. æ¨¡å‹å»ºç«‹éšæ®µ
```
ç®—æ³•é¸æ“‡ â†’ åƒæ•¸èª¿å„ª â†’ èšé¡åŸ·è¡Œ â†’ æ•ˆæœè©•ä¼°
```

### 3. æ¥­å‹™æ‡‰ç”¨éšæ®µ
```
ç¾¤çµ„è§£é‡‹ â†’ æ¨™ç±¤ç”Ÿæˆ â†’ ç­–ç•¥åˆ¶å®š â†’ ROIé ä¼°
```

## ğŸ“Š æ©Ÿå™¨å­¸ç¿’èšé¡ç®—æ³•è©³è§£

### K-Means èšé¡ç®—æ³•

#### ç†è«–åŸºç¤

K-Meansæ˜¯æœ€å¸¸ç”¨çš„èšé¡ç®—æ³•ï¼ŒåŸºæ–¼è·é›¢çš„ç›¸ä¼¼æ€§é€²è¡Œåˆ†ç¾¤ï¼š
> å°‡æ•¸æ“šåˆ†ç‚ºkå€‹ç¾¤çµ„ï¼Œä½¿å¾—ç¾¤å…§æ•¸æ“šé»ç›¡å¯èƒ½ç›¸ä¼¼ï¼Œç¾¤é–“æ•¸æ“šé»ç›¡å¯èƒ½ä¸åŒ

#### ç®—æ³•æ­¥é©Ÿ

1. **åˆå§‹åŒ–**: éš¨æ©Ÿé¸æ“‡kå€‹èšé¡ä¸­å¿ƒ
2. **åˆ†é…**: å°‡æ¯å€‹æ•¸æ“šé»åˆ†é…åˆ°æœ€è¿‘çš„èšé¡ä¸­å¿ƒ
3. **æ›´æ–°**: é‡æ–°è¨ˆç®—æ¯å€‹ç¾¤çµ„çš„ä¸­å¿ƒé»
4. **é‡è¤‡**: é‡è¤‡æ­¥é©Ÿ2-3ç›´åˆ°æ”¶æ–‚

**å¯¦ä½œç¨‹å¼ç¢¼**:
```python
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

def perform_kmeans_clustering(customer_data, k=4):
    # æ•¸æ“šæ¨™æº–åŒ–
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(customer_data)
    
    # åŸ·è¡ŒK-Means
    kmeans = KMeans(n_clusters=k, random_state=42)
    cluster_labels = kmeans.fit_predict(scaled_data)
    
    return cluster_labels, kmeans.cluster_centers_
```

**å„ªç¼ºé»åˆ†æ**:

| å„ªé» | ç¼ºé» |
|------|------|
| ç®—æ³•ç°¡å–®ï¼Œè¨ˆç®—æ•ˆç‡é«˜ | éœ€è¦é å…ˆæŒ‡å®šç¾¤æ•¸k |
| é©åˆçƒå½¢åˆ†å¸ƒçš„æ•¸æ“š | å°ç•°å¸¸å€¼æ•æ„Ÿ |
| çµæœå®¹æ˜“è§£é‡‹ | å‡è¨­ç¾¤çµ„å¤§å°ç›¸ä¼¼ |

### éšå±¤èšé¡ç®—æ³•

#### ç†è«–åŸºç¤

éšå±¤èšé¡é€šéé€æ­¥åˆä½µç›¸ä¼¼çš„ç¾¤çµ„ä¾†å»ºç«‹èšé¡æ¨¹ï¼š
> ä¸éœ€è¦é å…ˆæŒ‡å®šç¾¤æ•¸ï¼Œå¯ä»¥é€šéæ¨¹ç‹€åœ–é¸æ“‡æœ€ä½³åˆ†ç¾¤å±¤ç´š

#### é€£çµæ–¹æ³•æ¯”è¼ƒ

**Wardé€£çµ** (æ¨è–¦ä½¿ç”¨):
```python
from sklearn.cluster import AgglomerativeClustering

def perform_hierarchical_clustering(customer_data, n_clusters=4):
    hierarchical = AgglomerativeClustering(
        n_clusters=n_clusters, 
        linkage='ward'
    )
    cluster_labels = hierarchical.fit_predict(customer_data)
    return cluster_labels
```

| é€£çµæ–¹æ³• | ç‰¹é» | é©ç”¨å ´æ™¯ |
|----------|------|----------|
| Ward | æœ€å°åŒ–ç¾¤å…§æ–¹å·®ï¼Œç”¢ç”Ÿç·Šå¯†ç¾¤çµ„ | å®¢æˆ¶åˆ†ç¾¤ã€ç”¢å“åˆ†é¡ |
| Complete | æœ€å¤§è·é›¢é€£çµï¼Œç”¢ç”Ÿçƒå½¢ç¾¤çµ„ | å“è³ªæ§åˆ¶ã€é¢¨éšªç®¡ç† |
| Average | å¹³å‡è·é›¢é€£çµï¼Œå¹³è¡¡æ•ˆæœ | ä¸€èˆ¬åˆ†ç¾¤åˆ†æ |
| Single | æœ€å°è·é›¢é€£çµï¼Œæ˜“ç”¢ç”Ÿéˆç‹€ | ç¶²è·¯åˆ†æã€åœ°ç†åˆ†å¸ƒ |

### DBSCAN å¯†åº¦èšé¡

#### ç†è«–åŸºç¤

DBSCANåŸºæ–¼å¯†åº¦çš„èšé¡æ–¹æ³•ï¼Œèƒ½å¤ è‡ªå‹•ç¢ºå®šç¾¤æ•¸ä¸¦è™•ç†å™ªè²ï¼š
> å°‡å¯†åº¦ç›¸è¿‘çš„æ•¸æ“šé»æ­¸ç‚ºä¸€ç¾¤ï¼ŒåŒæ™‚è­˜åˆ¥å™ªè²é»

#### æ ¸å¿ƒåƒæ•¸

**eps (é„°åŸŸåŠå¾‘)**:
- å®šç¾©é»çš„é„°åŸŸç¯„åœ
- è¼ƒå°å€¼ç”¢ç”Ÿæ›´å¤šå°ç¾¤çµ„
- è¼ƒå¤§å€¼åˆä½µæ›´å¤šæ•¸æ“šé»

**min_samples (æœ€å°æ¨£æœ¬æ•¸)**:
- å½¢æˆæ ¸å¿ƒé»çš„æœ€å°é„°å±…æ•¸
- ç¶“é©—æ³•å‰‡ï¼š2Ã—ç‰¹å¾µç¶­åº¦

**å¯¦ä½œç¨‹å¼ç¢¼**:
```python
from sklearn.cluster import DBSCAN

def perform_dbscan_clustering(customer_data, eps=0.5, min_samples=5):
    dbscan = DBSCAN(eps=eps, min_samples=min_samples)
    cluster_labels = dbscan.fit_predict(customer_data)
    
    # åˆ†æçµæœ
    n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)
    n_noise = list(cluster_labels).count(-1)
    
    return cluster_labels, n_clusters, n_noise
```

## ğŸ› ï¸ ç‰¹å¾µå·¥ç¨‹å¯¦æˆ°æŒ‡å—

### 1. RFMç‰¹å¾µæ“´å±•

#### åŸºç¤RFMè¨ˆç®—
```sql
-- æ“´å±•RFMç‰¹å¾µè¨ˆç®—
WITH customer_metrics AS (
    SELECT 
        customer_id,
        -- åŸºç¤RFM
        DATEDIFF(CURRENT_DATE, MAX(order_date)) as recency_days,
        COUNT(DISTINCT order_id) as frequency_90d,
        SUM(order_amount) as monetary_90d,
        
        -- æ“´å±•ç‰¹å¾µ
        AVG(order_amount) as avg_order_value,
        STDDEV(order_amount) as order_amount_std,
        COUNT(DISTINCT product_category) as category_diversity,
        
        -- æ™‚é–“ç‰¹å¾µ
        DATEDIFF(MAX(order_date), MIN(order_date)) as customer_lifespan,
        COUNT(DISTINCT DATE_FORMAT(order_date, '%Y-%m')) as active_months,
        
        -- è¡Œç‚ºç‰¹å¾µ
        SUM(CASE WHEN order_channel = 'mobile' THEN 1 ELSE 0 END) / COUNT(*) as mobile_ratio,
        SUM(CASE WHEN discount_amount > 0 THEN 1 ELSE 0 END) / COUNT(*) as discount_usage_ratio
        
    FROM orders o
    WHERE order_date >= DATE_SUB(CURRENT_DATE, INTERVAL 12 MONTH)
    GROUP BY customer_id
)
SELECT 
    customer_id,
    recency_days,
    frequency_90d,
    monetary_90d,
    avg_order_value,
    category_diversity,
    customer_lifespan,
    mobile_ratio,
    discount_usage_ratio,
    
    -- å®¢æˆ¶åƒ¹å€¼æŒ‡æ¨™
    (monetary_90d / NULLIF(customer_lifespan, 0)) * 365 as annual_value,
    (frequency_90d / NULLIF(active_months, 0)) as monthly_frequency,
    
    -- ç©©å®šæ€§æŒ‡æ¨™
    CASE 
        WHEN order_amount_std / NULLIF(avg_order_value, 0) < 0.3 THEN 'Stable'
        WHEN order_amount_std / NULLIF(avg_order_value, 0) < 0.7 THEN 'Variable'
        ELSE 'Volatile'
    END as spending_pattern
    
FROM customer_metrics;
```

### 2. ä¸»æˆåˆ†åˆ†æ (PCA)

PCAç”¨æ–¼é™ç¶­å’Œç‰¹å¾µé¸æ“‡ï¼Œå¹«åŠ©ï¼š
- æ¸›å°‘ç‰¹å¾µç¶­åº¦
- å»é™¤ç‰¹å¾µé–“çš„ç›¸é—œæ€§
- ä¿ç•™æœ€é‡è¦çš„ä¿¡æ¯
- è¦–è¦ºåŒ–é«˜ç¶­æ•¸æ“š

```python
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

def perform_pca_analysis(customer_features, n_components=2):
    """
    åŸ·è¡Œä¸»æˆåˆ†åˆ†æ
    """
    # æ•¸æ“šæ¨™æº–åŒ–
    scaler = StandardScaler()
    scaled_features = scaler.fit_transform(customer_features)
    
    # åŸ·è¡ŒPCA
    pca = PCA(n_components=n_components)
    principal_components = pca.fit_transform(scaled_features)
    
    # åˆ†æçµæœ
    explained_variance_ratio = pca.explained_variance_ratio_
    
    return {
        'principal_components': principal_components,
        'explained_variance_ratio': explained_variance_ratio,
        'pca_model': pca
    }
```

## ğŸ“ˆ èšé¡æ•ˆæœè©•ä¼°

### 1. è¼ªå»“ä¿‚æ•¸ (Silhouette Score)

è¼ªå»“ä¿‚æ•¸è¡¡é‡æ¯å€‹æ•¸æ“šé»èˆ‡å…¶æ‰€åœ¨ç¾¤çµ„çš„ç›¸ä¼¼åº¦ï¼Œç¯„åœåœ¨-1åˆ°1ä¹‹é–“ï¼š
- æ¥è¿‘1ï¼šæ•¸æ“šé»èˆ‡å…¶ç¾¤çµ„å¾ˆç›¸ä¼¼ï¼Œèˆ‡å…¶ä»–ç¾¤çµ„å¾ˆä¸åŒ
- æ¥è¿‘0ï¼šæ•¸æ“šé»åœ¨å…©å€‹ç¾¤çµ„çš„é‚Šç•Œä¸Š
- æ¥è¿‘-1ï¼šæ•¸æ“šé»å¯èƒ½è¢«åˆ†é…åˆ°éŒ¯èª¤çš„ç¾¤çµ„

### 2. Davies-BouldinæŒ‡æ•¸

Davies-BouldinæŒ‡æ•¸è©•ä¼°ç¾¤çµ„é–“çš„åˆ†é›¢åº¦å’Œç¾¤çµ„å…§çš„ç·Šå¯†åº¦ï¼Œå€¼è¶Šå°è¡¨ç¤ºèšé¡æ•ˆæœè¶Šå¥½ã€‚

### 3. è‚˜éƒ¨æ³•å‰‡ (Elbow Method)

é€šéè§€å¯ŸSSEéš¨Kå€¼è®ŠåŒ–çš„æ›²ç·šï¼Œæ‰¾å‡º"è‚˜éƒ¨"é»ä½œç‚ºæœ€ä½³Kå€¼ã€‚

## ğŸ·ï¸ å®¢æˆ¶ç¾¤çµ„æ¨™ç±¤èˆ‡ç­–ç•¥åˆ¶å®š

### ç¾¤çµ„æ¨™ç±¤ç”Ÿæˆ

åŸºæ–¼èšé¡çµæœï¼Œè‡ªå‹•ç”Ÿæˆæ¥­å‹™å‹å¥½çš„å®¢æˆ¶æ¨™ç±¤ï¼š

1. **ğŸ† å† è»å®¢æˆ¶**: é«˜é »é«˜é¡è³¼è²·ï¼Œå“ç‰Œå¿ èª åº¦é«˜
2. **ğŸ’ å¿ èª å®¢æˆ¶**: ç©©å®šè³¼è²·ï¼Œåƒ¹æ ¼æ•æ„Ÿåº¦ä½  
3. **ğŸŒŸ æ½›åŠ›å®¢æˆ¶**: è³¼è²·é »ç‡é©ä¸­ï¼Œæœ‰æå‡ç©ºé–“
4. **ğŸ˜´ ä¼‘çœ å®¢æˆ¶**: é•·æ™‚é–“æœªè³¼è²·ï¼Œéœ€è¦å–šé†’ç­–ç•¥
5. **ğŸ†• æ–°å®¢æˆ¶**: æ–°è¨»å†Šæˆ–é¦–æ¬¡è³¼è²·çš„å®¢æˆ¶

### ROIé ä¼°ç³»çµ±

ç‚ºæ¯å€‹å®¢æˆ¶ç¾¤çµ„é ä¼°ç‡ŸéŠ·æŠ•è³‡å›å ±ï¼š

```python
def calculate_roi_projection(segment_info, investment=1000):
    """
    è¨ˆç®—ç¾¤çµ„ROIé ä¼°
    """
    conversion_rates = {
        'ğŸ† å† è»å®¢æˆ¶': 0.85,
        'ğŸ’ å¿ èª å®¢æˆ¶': 0.65,
        'ğŸŒŸ æ½›åŠ›å®¢æˆ¶': 0.45,
        'ğŸ†• æ–°å®¢æˆ¶': 0.35,
        'ğŸ˜´ ä¼‘çœ å®¢æˆ¶': 0.15
    }
    
    avg_order_values = {
        'ğŸ† å† è»å®¢æˆ¶': 150,
        'ğŸ’ å¿ èª å®¢æˆ¶': 120,
        'ğŸŒŸ æ½›åŠ›å®¢æˆ¶': 80,
        'ğŸ†• æ–°å®¢æˆ¶': 70,
        'ğŸ˜´ ä¼‘çœ å®¢æˆ¶': 40
    }
    
    segment_label = segment_info['label']
    customer_count = segment_info['count']
    
    conversion_rate = conversion_rates.get(segment_label, 0.2)
    avg_order_value = avg_order_values.get(segment_label, 60)
    
    projected_customers = customer_count * conversion_rate
    projected_revenue = projected_customers * avg_order_value
    roi = ((projected_revenue - investment) / investment) * 100
    
    return {
        'projected_revenue': projected_revenue,
        'roi': roi,
        'conversion_rate': conversion_rate * 100
    }
```

## ğŸ“Š å¯¦æˆ°æ¡ˆä¾‹åˆ†æ

### é›»å•†å¹³å°å®¢æˆ¶åˆ†ç¾¤æ¡ˆä¾‹

æŸé›»å•†å¹³å°é€šéå®¢æˆ¶åˆ†ç¾¤åˆ†æï¼š

1. **æ•¸æ“šæº–å‚™**: æ”¶é›†12å€‹æœˆçš„äº¤æ˜“æ•¸æ“šã€è¡Œç‚ºæ•¸æ“šã€ç”¨æˆ¶ç•«åƒ
2. **ç‰¹å¾µå·¥ç¨‹**: æ§‹å»ºRFMç‰¹å¾µã€è¡Œç‚ºç‰¹å¾µã€åå¥½ç‰¹å¾µ
3. **æ¨¡å‹å»ºç«‹**: æ¯”è¼ƒK-Meansã€éšå±¤èšé¡ã€DBSCANæ•ˆæœ
4. **ç¾¤çµ„åˆ†æ**: è­˜åˆ¥å‡º5å€‹ä¸»è¦å®¢æˆ¶ç¾¤çµ„
5. **ç­–ç•¥åˆ¶å®š**: ç‚ºæ¯å€‹ç¾¤çµ„åˆ¶å®šå·®ç•°åŒ–ç‡ŸéŠ·ç­–ç•¥

**å¯¦æ–½çµæœ**:
- æ•´é«”ç‡Ÿæ”¶å¢é•·28%
- å®¢æˆ¶æµå¤±ç‡é™ä½35%
- ç‡ŸéŠ·ROIå¾85%æå‡è‡³135%
- å®¢æˆ¶æ»¿æ„åº¦å¾7.2åˆ†æå‡è‡³8.6åˆ†

## ğŸš€ å¯¦è¸å»ºè­°

### 1. å¯¦æ–½è·¯ç·šåœ–

- **ç¬¬1-2é€±**: æ•¸æ“šæ”¶é›†å’Œå“è³ªè©•ä¼°
- **ç¬¬3-4é€±**: ç‰¹å¾µå·¥ç¨‹å’Œé è™•ç†
- **ç¬¬5-6é€±**: æ¨¡å‹å»ºç«‹å’Œåƒæ•¸èª¿å„ª
- **ç¬¬7-8é€±**: æ¥­å‹™æ‡‰ç”¨å’Œç­–ç•¥åˆ¶å®š
- **ç¬¬9-12é€±**: æ•ˆæœé©—è­‰å’ŒæŒçºŒå„ªåŒ–

### 2. å¸¸è¦‹æŒ‘æˆ°

#### æ•¸æ“šå“è³ªå•é¡Œ
- å»ºç«‹æ•¸æ“šå“è³ªæª¢æŸ¥æ©Ÿåˆ¶
- è™•ç†ç¼ºå¤±å€¼å’Œç•°å¸¸å€¼
- ç¢ºä¿æ•¸æ“šä¸€è‡´æ€§å’Œæº–ç¢ºæ€§

#### ç‰¹å¾µé¸æ“‡å›°é›£
- ä½¿ç”¨ç‰¹å¾µé‡è¦æ€§åˆ†æ
- é€²è¡Œç›¸é—œæ€§åˆ†æ
- çµåˆæ¥­å‹™çŸ¥è­˜é¸æ“‡ç‰¹å¾µ

#### æ¨¡å‹è§£é‡‹å›°é›£
- ç”Ÿæˆç¾¤çµ„ç‰¹å¾µå ±å‘Š
- ä½¿ç”¨è‡ªç„¶èªè¨€æè¿°
- æä¾›å¯è¦–åŒ–åˆ†æçµæœ

### 3. æŒçºŒå„ªåŒ–

- å®šæœŸç›£æ§æ¨¡å‹æ€§èƒ½
- æª¢æ¸¬æ•¸æ“šæ¼‚ç§»å’Œæ¦‚å¿µæ¼‚ç§»
- æ ¹æ“šæ¥­å‹™åé¥‹èª¿æ•´ç­–ç•¥
- å»ºç«‹A/Bæ¸¬è©¦é©—è­‰æ©Ÿåˆ¶
  FROM orders
  WHERE order_date >= CURRENT_DATE - INTERVAL '90' DAY
  GROUP BY customer_id
)
SELECT customer_id,
       DATEDIFF('day', last_date, CURRENT_DATE) AS recency_days,
       frequency_90d,
       monetary_90d
FROM orders_90d;
```

## 2. ç‰¹å¾µç¸®æ”¾èˆ‡é™ç¶­ï¼ˆå¯é¸ï¼‰
- StandardScaler/MinMaxScaler
- PCA/UMAP åƒ…ç”¨æ–¼è¦–è¦ºåŒ–ï¼Œä¸å¿…ä½œç‚ºè¨“ç·´å¿…è¦æ­¥é©Ÿ

## 3. èšé¡å»ºæ¨¡
```python
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.metrics import silhouette_score, davies_bouldin_score

X = df[features].values
X = StandardScaler().fit_transform(X)

# KMeans
scores = []
for k in range(2, 11):
    km = KMeans(n_clusters=k, random_state=42, n_init='auto').fit(X)
    labels = km.labels_
    sil = silhouette_score(X, labels)
    dbi = davies_bouldin_score(X, labels)
    scores.append((k, sil, dbi))

best_k = max(scores, key=lambda x: x[1])[0]
km = KMeans(n_clusters=best_k, random_state=42, n_init='auto').fit(X)
labels = km.labels_
```

## 4. åˆ†ç¾¤å“è³ªèˆ‡å‘½å
- ä»¥ Silhouetteã€DBI èˆ‡æ¥­å‹™å¯è§£é‡‹æ€§ç¶œåˆæ±ºç­–
- ç¾¤çµ„å‘½åè¦å‰‡ï¼šé«˜åƒ¹å€¼å¿ èª ã€æ½›åŠ›æ–°å®¢ã€åƒ¹æ ¼æ•æ„Ÿã€æ²‰ç¡å–šé†’â€¦

```python
import pandas as pd

df['cluster'] = labels
profile = df.groupby('cluster')[features].mean()
# ä¾æ“šç‰¹å¾µå¼·å¼±ã€å æ¯”èˆ‡åƒ¹å€¼æŒ‡æ¨™ç”Ÿæˆç¾¤çµ„æ¨™ç±¤
```

## 5. è¦–è¦ºåŒ–å»ºè­°ï¼ˆEChartsï¼‰
- 2D/3D æ•£é»ï¼ˆPCA/UMAPï¼‰
- é›·é”åœ–ï¼šç¾¤çµ„ä¸­å¿ƒç‰¹å¾µ
- ç’°åœ–ï¼šç¾¤çµ„å æ¯”
- ç®±å½¢åœ–ï¼šç¾¤å…§åˆ†ä½ˆå·®ç•°

## 6. ç­–ç•¥è½åœ°
- é‡å°æ¯ç¾¤ï¼šä¿ƒæ´»/å¬å›ã€æ¨è–¦ã€åŠ åƒ¹ã€äº¤å‰éŠ·å”®
- å»ºç«‹ç­–ç•¥å¡ç‰‡ï¼šç›®æ¨™å®¢ç¾¤ã€è¨Šæ¯ä¸»é¡Œã€æ¸ é“ã€é æœŸ upliftã€KPI

## 7. é©—è­‰èˆ‡è¿­ä»£
- A/B æˆ–æ­·å²å›æº¯è©•ä¼°ç­–ç•¥æ•ˆæœ
- é€±æœŸæ€§é‡è·‘èšé¡ï¼Œè¿½è¹¤ç¾¤çµ„æ¼‚ç§»

---

- ç›¸é—œæª”æ¡ˆï¼š
  - `docs/data-science-lab/MODULE-05.md`
  - `docs/data-science-lab/modules/module-05-customer-segmentation-lab.md` 